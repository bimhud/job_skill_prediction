{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert-xmlc.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNWIX/qfUchyojKY2jNhJHU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction\n","\n","This document demonstrates the concept to build a job skill/job requirement classifier to classify a given JD with a set of relevant skills/requirements.\n","\n","- The dataset: \n","  * **ads-50k.json**: 50K job description from Seek with unstructured text contents as HTML format\n","  * **ads-50k-events.csv**: a many-to-many relationship, reflecting the events for candidates applying for job advertised.\n","- The requirements:\n","  * **Provide analyses and provide a solution** that would allow to annotate ads with skills, responsibilities and/or other requirements for a successful candidate to perform in their role.\n","\n","\n","\n"],"metadata":{"id":"W7gpezF5YFjT"}},{"cell_type":"markdown","source":["# Observations\n","\n","1. Each JD is provided as page of unstructured text, there isn't available labels or tags related to skills/reponsibilities/requirements\n","2. Text from JD provides rich info about the jobs, skills required, responsibilities and requirements. The in-line texts extracted from the JD are useful and could be considered as candicates for skills/reponsibilites/requirements\n","3. Majority of JDs have a similar format with starting with a **section header** and following with **bullet points** listing relevents. This information is useful to identify the role of each part of a JD.\n","4. Following the header of each section, it is possible to identify the key info mentioned\n","5. A large number of JD have the free style of writing without the structure of **section header** and **bullet points**\n","6. The relationship between job candidates and job adds could provide further infomation about which job ads are relevant on the same professionals or industries. This info is useful for grouping and identifying characteristics of the groups.\n","7. To simplify, skills/reponsibilites/requirements from now could be called as tags or labels\n","\n","# Analysis\n","The problem of automatically annotating JDs with labels could be formulated using two ways:\n","\n","1. A pattern-based approach\n","  - With this approach, patterns are designed and passing through JD to extract candidates of labels\n","  - Using purely rules to extract the high quality labels. These labels could be used as seeds to assist to (1) design new pattern or (2) extract further other label candicates.\n","  - Existing list of labels might partially available from Seek/Linkedin or other external sources. These could be used to increase the quality of pattern and the extraction\n","  - Using the associated between JD text and the extracted labels could be used to create ground-true training data to train a supervised model for annotation.\n","\n","2. A supervised learning approach - eXtreme Multi-label Classification\n","  - Using data from (1), each JD is provided with a set of labels using pattern and existing knowledge.\n","  - The data is used as the ground true to train a multi-label classifier to assign tags to JDs.\n","\n","# Proposal\n","\n","With the purpose of demonstrate the concept (proof-of-concept), the following proposal has been undertaken:\n","\n","1. Using a pattern-based approach to identify unstructured text where labels are listed. This could be done by following the high quality job ads where there is a right structure of writing using **\"section header\"**\n","\n","2. Using a pattern-based approach to identify the **good quality key-phrases** that are mentioned from the text in (1). The redundancy/co-location is used to qualify the extraction.\n","Due to the limitted scope of this POC, no external data has been used. However, the quality of extraction could be significantly improved if external data is available such as a list of available skills from the job market.\n","\n","3. Data from (1) and (2) are used to create a ground true dataset to feed into a multi-label classifier for the annotation.\n","\n","4. A extreme multi-label classifier is proposed to build, which takes into the advantage of pre-train model on text domain and adapt to a multi-label classification on job-ad text domain. This model is build based on BERT pre-trained models.\n","\n","# Discussion\n","1. The proposed solution demonstrates the ability to build a multi-label skill classifier from the scratch using bootstraping approach\n","2. The solution has the ability to annotate labels what are directly from the text or from a similar JD\n","3. The quality of the ground true dataset could be improved by using existing/available list of skills.  Rule-based or semantic matching could play an important role to support this.\n","4. The relationship between labels could be utilised to improve the quality of label selection. This could be done by using the relationship between job ads via resume and job ads event logs.\n","5. This solution is scalable to support batch processing and real-time prediction and lays a foundation for downstream applications.\n","6. There are many directions that could be improved from this POC to improve the accuracy\n"],"metadata":{"id":"FzNtO0mUakmy"}},{"cell_type":"code","source":["# Analysis and Proposal\n"],"metadata":{"id":"wDkS7GZ3abn0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Map Google Drive to Notebook to save data for re-producible"],"metadata":{"id":"-hsiINUL6Yaz"}},{"cell_type":"code","source":["# Map the Google drive to the notebook colab to store the results\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/git/\n","\n","# Check out our git repo tailor for seek-dataset \n","#!git checkout https://job-skill-prediction\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V8fHjhTkN69J","executionInfo":{"status":"ok","timestamp":1643106456191,"user_tz":-660,"elapsed":22564,"user":{"displayName":"Dat Huynh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09147954537197073565"}},"outputId":"cbc54b30-6d2f-4d87-955b-3f1421a2d80c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/git\n"]}]},{"cell_type":"markdown","source":["# [2. Install dependencies for ground-true dataset extraction](#depencency_1)\n","\n","Dependencies for ground-true dataset extraction. Using BeautifulSoup to parse JD. Using textacy (spacy) for extracting key-phrases"],"metadata":{"id":"Qr6RpKmL60TU"}},{"cell_type":"code","source":["!pip install beautifulsoup4\n","!pip install textacy\n","!python -m spacy download en_core_web_sm\n","\n","import json\n","import re\n","from bs4 import BeautifulSoup\n","import traceback\n","from tqdm import tqdm\n","import textacy\n","from textacy import extract\n","import pandas as pd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zh7tsks-6y6K","executionInfo":{"status":"ok","timestamp":1642963770672,"user_tz":-660,"elapsed":30104,"user":{"displayName":"Dat Huynh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09147954537197073565"}},"outputId":"6f16634e-c615-47e5-d8c2-efee3233d30a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n","Requirement already satisfied: textacy in /usr/local/lib/python3.7/dist-packages (0.11.0)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.19.5)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.6.3)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.4.1)\n","Requirement already satisfied: cytoolz>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.11.2)\n","Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.21.3)\n","Requirement already satisfied: pyphen>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.12.0)\n","Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.1.0)\n","Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.62.3)\n","Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.2.4)\n","Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (3.2.1)\n","Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.23.0)\n","Requirement already satisfied: jellyfish>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.9.0)\n","Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from cytoolz>=0.10.1->textacy) (0.11.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2.10)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.0.6)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.4.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.9.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (57.4.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.11.3)\n","Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.10.0.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.3.0)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.4.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.4.2)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.0.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.0.6)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.8.2)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (8.0.13)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.0.6)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.6.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.0.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (21.3)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0.0->textacy) (3.7.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.0.0->textacy) (3.0.6)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0.0->textacy) (5.2.1)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0.0->textacy) (2.0.1)\n","Collecting en-core-web-sm==3.2.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n","\u001b[K     |████████████████████████████████| 13.9 MB 23.4 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n","Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"s-vvtjfl8cVz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# [3. Process raw dataset and generate candidate skills/requirements](#preprocess_raw_dataset)\n","\n","This process is to apply rule-based and heuristic approach to select the good quality set of skills/requirements/responsibilities. Given a JD, the following process are undertaken:\n","1. Scan the JD and select the **section headers** that are potentially the heading of skills and requirements\n","2. Extract **paragraphs of text** that are potentially contains skills and requirements\n","3. Apply **key-phrase extraction** to identify the significant mentions from the listed skill/requirement texts.\n","4. Select the** final set of skills/requirements/responsibilities** that are considered as the 'tags' or 'labels' for the JD. \n","\n","After this process, each JD will have a list of labels. Due to the strict selection process, these could be considered as the '**ground true'** information to support for the task of automatically identifying skills from a given JD. This task is formulated as a eXtreme Multi-label Classification problem, which is presented in the next section.\n"],"metadata":{"id":"cYH_cWSs7X5h"}},{"cell_type":"markdown","source":["## [3.1 Define helper functions](#helper_function)\n","\n","These functions to support the processing below it"],"metadata":{"id":"pm2OVnzG1ud4"}},{"cell_type":"code","source":["import re\n","\n","def get_file_size(raw_dataset_path):\n","  size = 0\n","  with open(raw_dataset_path,\"rt\") as f:\n","    for line in f:\n","      size+=1\n","  return size\n","\n","\n","def clean_html_text(text):\n","  \"\"\"\n","    clean all html tag, replace with '.' if needed.\n","  \"\"\"\n","  if text is None or len(text)<=0:\n","    return \"\"\n","\n","  # remove html tag\n","  text = re.sub(\"<\\/strong>\",\"\",text.lower()) \n","  text = re.sub(\"<\\w+>\",\"\",text)\n","  # handling those without ending line with 'dot'\n","  text = re.sub(\"(\\w)</p>\",\"\\\\1. \",text)\n","  text = re.sub(\"(\\w)</li>\",\"\\\\1. \",text)\n","  text = re.sub(\"(\\w)<br/>\",\"\\\\1. \",text)\n","  # remove the remaining html tag\n","  text = re.sub(\"<\\/\\w+>\",\"\",text)\n","  text = re.sub(\"<\\w+\\/>\",\"\",text)\n","\n","  # remove 'tab'\n","  text = re.sub(\"\\t\",\" \", text)\n","\n","  # remove special html characters\n","  text = re.sub(\"\\&nbsp;\",\" \", text)\n","  text = re.sub(\"\\&amp;\",\"and\", text)\n","  text = re.sub(\"\\n\",\" \", text)\n","  text = re.sub(\"&rsquo;\",\" \", text)\n","  text = re.sub(\"&rdquo;\",\" \", text)\n","  text = re.sub(\"&ldquo;\",\" \", text)\n","  return text\n","\n","\n","# test with sample text\n","text=\"\"\"\n","</p>\n","<p>To be considered for this position, you will require:</p>\n","<ul>\n","<li>Proven experience in estimation within a construction/building environment</li>\n","<li>Degree qualified or equivalent (Degree/Diploma) preferably in Quantity Surveying</li>\n","<li>Demonstrated ability to analyse, evaluate and interpret a range of complex and technical documents, including relevant regulatory, legislative, and licensing requirements, codes and standards, plans, drawings and specifications, invitations to tender, contracts and procurement reports, and bills of quantities</li>\n","<li>Exhibit excellent communication and interpersonal skills</li>\n","<li>Experience in managing estimating teams (if applying for a Manager role)</li>\n","<li>Cost Planning experience and capability (relevant to building projects)</li>\n","<li>Ability to measure documentation provided by client during RFT to create a Bill of Quantities</li>\n","<li>Understand how to fill in the &ldquo;gaps&rdquo; in tender documentation</li>\n","<li>Can create a Builder&rsquo;s Bill of Quantities from the measure</li>\n","<li>Understands how to use Cost X and OST to measure and create the Bill</li>\n","<li>Has experience in using BIM models to measure Bills (preferable)</li>\n","</ul>\n","<p>The successful candidate will be rewarded with the opportunity to work on diverse and challenging projects and on-going professional development within a supportive and encouraging team environment.</p>\n","<p><em>We support diversity in the workplace. Women, Aboriginal and Torres Strait Islanders and people with a multicultural background are strongly encouraged to apply.</em></p>\n","<p><em>Please note: This role is being sourced through CPB Contractors directly and we will not accept applications via external recruitment agencies.</em></p></HTML>\n","\"\"\"\n","clean_html_text(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"GEFmk2DROjyP","executionInfo":{"status":"ok","timestamp":1642975844964,"user_tz":-660,"elapsed":421,"user":{"displayName":"Dat Huynh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09147954537197073565"}},"outputId":"c45dbed2-b991-4265-fcb3-ae31a2862cec"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'  to be considered for this position, you will require:  proven experience in estimation within a construction/building environment.  degree qualified or equivalent (degree/diploma) preferably in quantity surveying.  demonstrated ability to analyse, evaluate and interpret a range of complex and technical documents, including relevant regulatory, legislative, and licensing requirements, codes and standards, plans, drawings and specifications, invitations to tender, contracts and procurement reports, and bills of quantities.  exhibit excellent communication and interpersonal skills.  experience in managing estimating teams (if applying for a manager role) cost planning experience and capability (relevant to building projects) ability to measure documentation provided by client during rft to create a bill of quantities.  understand how to fill in the  gaps  in tender documentation.  can create a builder s bill of quantities from the measure.  understands how to use cost x and ost to measure and create the bill.  has experience in using bim models to measure bills (preferable)  the successful candidate will be rewarded with the opportunity to work on diverse and challenging projects and on-going professional development within a supportive and encouraging team environment. we support diversity in the workplace. women, aboriginal and torres strait islanders and people with a multicultural background are strongly encouraged to apply. please note: this role is being sourced through cpb contractors directly and we will not accept applications via external recruitment agencies. '"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["## [3.2 Enrich the current raw dataset with extras info from the extraction](#dataset_enrichment)\n","\n","1. This process is to extract a list of labels from a given JD\n","2. The output from this is a dataset file with additional data fields such as \"skill_content\", \"skill_list\", \"skill_with_weight\"\n","  - **skill_content**: a list of pairs of **section header** and **text contents** right below it.\n","  - **skill_list**: a list of labels extracted from a coresponding **section header**. One JD mights have multiple section, hence having multiple list of skills/requirements/responsibilities\n","  - **skill_with_weight**: a final list of labels that are aggreated from multiple sections from a JD. These also have their own weights\n","3.  Checkout the **ads-50k-with-skills.json** for details about the extraction for each JD. Only those JD having high quality info are selected to be used a ground-true, the rest is ignored from the training dataset."],"metadata":{"id":"9JxaIU8o11u0"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"0E_C634C6zFk"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_-WDxCmW8Mz","outputId":"247a565f-66c4-4428-d628-64f2b4c531b0"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 49753/50000 [16:50<00:05, 49.05it/s]"]}],"source":["\n","# Define a list of keywords that potentially belong to a header of relevant sections from the JD. To demonstrate the concept, these are selected. \n","# This list could be expanded based on further observation\n","skill_keyword_list = [\"the role\", # <strong>the role includes:</strong> or <strong>about the role:</strong>\n","                      \"requirement\", #<strong>the role includes:</strong>\n","                      \"what we require\", \n","                      \"about you\", \n","                      \"responsibili\", \n","                      \"need\", \n","                      \"skill\", \n","                      \"experience\", \n","                      \"candidate\"\n","                      ]\n","raw_dataset_path = \"seek_training_data_generation/ads-50k.json\" #input raw dataset\n","extracted_dataset_path = \"seek_training_data_generation/ads-50k-with-skills.json\" #output enriched dataset\n","\n","\n","size = get_file_size(raw_dataset_path)\n","\n","f = open(raw_dataset_path)\n","g = open(extracted_dataset_path, 'a')\n","\n","# Process extract skill text for each JD\n","for idx,line in tqdm(enumerate(f),total=size):\n","  data = json.loads(line)\n","  content = data[\"content\"]\n","\n","  #print(f\"-------Processing id {idx}--------\\n\")\n","\n","  #1. IDENTIFY SKILL CONTENT WHICH INCLUDE SKILL HEADER AND SKILL PARAGRAPH\n","  soup = BeautifulSoup(content, 'html.parser')\n","  \n","  skill_heading_text_pairs = [] # A list of tuple which contains the heading text and the skill paragraphs\n","\n","  pair_dict= {} # contact the pair of headers where the text in the middle to be extracted\n","  header_list = [] # containing a list of header from the JD\n","  result_list = [] # containing the selected list of headers that are potentially contains kills and requirements\n","\n","  for strong in soup.find_all('strong'): #identify heading\n","    text = strong.text\n","    \n","    if len(text.split()) >=5: # Any header with more than 5 words tends to be a false alarm\n","      continue\n","\n","    header_list.append(text)\n","    # Create a mapping to mark the start and end of a block \n","    if len(header_list)>=2:\n","      pair_dict[header_list[-2]] = header_list[-1]\n","\n","    \n","    for keyword in skill_keyword_list: #check if a skills/requirements is a part of the header\n","      if keyword in text.lower():\n","        result_list.append(text)\n","        break #\n","  \n","\n","  for keyword in result_list: #obtain skill paragraph from selected heading text\n","    rs = None\n","    if keyword in pair_dict:\n","      pattern = f\"<strong>{keyword}</strong>([\\w\\W]+)<strong>{pair_dict[keyword]}</strong>\"\n","    else:\n","      pattern = f\"<strong>{keyword}</strong>([\\w\\W]+)\"\n","    \n","    #print(f\"pattern: {pattern}\")\n","\n","    try:\n","      rs = re.search(pattern, content)\n","    except:\n","      pass\n","\n","    if rs is not None: \n","      skill_text = rs.group(1)\n","      skill_text = clean_html_text(skill_text)\n","\n","      skill_heading = clean_html_text(keyword)\n","      if len(skill_heading)>0 and len(skill_text)>0:\n","        skill_heading_text_pairs.append((skill_heading,skill_text))\n","        #print(f\"{skill_heading}={skill_text}\")\n","\n","  data[\"skill_content\"] = skill_heading_text_pairs #obtain skill content for each JD\n","\n","  # 2. EXTRACTING KEYWORD FROM TEXT REPRESENTED FOR SKILLS/REQUIREMENTS\n","\n","  data[\"skill_list\"] = []\n","  skill_list = []\n","  skill_value_list = []\n","  for skill_pair in  skill_heading_text_pairs:\n","    # for each pair of \"skill_heading\" and skill_paragraph, extract list of keywords using Spacy\n","    heading = skill_pair[0]\n","    text = skill_pair[1]\n","    doc = textacy.make_spacy_doc(text,\"en_core_web_sm\")\n","    \n","    #print(f\"heading={heading}\")\n","    #print(f\"text={text}\")\n","    skill_tuples = kt.textrank(doc, normalize=\"lemma\", topn=10)\n","    #print(f\"skills={skill_tuples}\\n\")\n","\n","    skill_list_per_heading = [v[0] for v in skill_tuples]\n","    \n","    # Add a list of skills based on each heading\n","    data[\"skill_list\"].append((heading,skill_list_per_heading))\n","\n","    #combine from all heading\n","    if len(skill_list) <= 0:\n","      skill_list = [v[0] for v in skill_tuples]\n","      skill_value_list = [v[1] for v in skill_tuples]\n","    else:\n","      skill_list.extend([v[0] for v in skill_tuples])\n","      skill_value_list.extend([v[1] for v in skill_tuples])\n","\n","  # 3. CALCULATE AND PROVIDE THE FINAL OUTPUT DATA\n","  \n","  # Calculate the aggreation if there is duplication \n","  df = pd.DataFrame({\"skill_name\":skill_list, 'skill_value': skill_value_list})\n","  df = df.groupby(\"skill_name\").sum().reset_index().sort_values(\"skill_value\",ascending=False)\n","\n","  # Add skill list with weights\n","  data[\"skill_with_weight\"] = df.values.tolist()\n","\n","  #print(data[\"skill_list\"])\n","  #print(data[\"skill_with_weight\"])\n","\n","  del data[\"content\"]\n","  del data[\"metadata\"]\n","  del data[\"abstract\"]\n","\n","  g.write(json.dumps(data))\n","  g.write(\"\\n\")\n","\n","  #if idx >=10:\n","  #  break\n","\n","f.close()\n","g.close()\n"]},{"cell_type":"markdown","source":["#[4. Create ground-true dataset for support to train a minni-supervised skill classifier](#create_ground_true_dataset)\n","\n","In this section, a ground-true dataset is created from the original raw dataset. This is done by selecting those with good quality of label extraction.\n","\n","The following steps are undertaken:\n","1. Create a **global ground-true dataset** based on the processed JD with skills (labels). \n","**Location**=bert_extreme_multilabel_classification/pybert/dataset/seek_dataset/dataset.csv\n","2. Generate **train/validation datasets** to support to train a multi-skill classifier. \n","**Location**=bert_extreme_multilabel_classification/pybert/dataset/seek_dataset/{seek_dataset_train.pkl,seek_dataset_valid.pkl}\n","3. Generate **a list of global skills/requirements** from the given seek dataset. \n","**Location**=bert_extreme_multilabel_classification/pybert/dataset/seek_dataset/skill_list.csv"],"metadata":{"id":"MLt8OgtD3Oma"}},{"cell_type":"code","source":["import numpy as np\n","def remove_outlier(df):\n","  d = df[df[\"skill_value_count\"]>=10]\n","  #d = d[d[\"skill_value_sum\"]>=2]\n","  return d"],"metadata":{"id":"EQp0Ie3rbe6A","executionInfo":{"status":"ok","timestamp":1642975860555,"user_tz":-660,"elapsed":965,"user":{"displayName":"Dat Huynh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09147954537197073565"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["extracted_dataset_path = \"seek_training_data_generation/ads-50k-with-skills.json\"\n","label_path = \"bert_extreme_multilabel_classification/pybert/dataset/seek_dataset/skill_list.csv\"\n","output_dataset_path = \"bert_extreme_multilabel_classification/pybert/dataset/seek_dataset/dataset.csv\"\n","\n","\n","\n","\n","size = get_file_size(extracted_dataset_path)\n","\n","f = open(extracted_dataset_path, 'rb')\n","\n","# Process extract skill list\n","df_skills = None\n","for idx,line in tqdm(enumerate(f),total=size):\n","  if len(line)==1: #b'\\n'\n","    continue\n","  try:\n","    data = json.loads(line)\n","  except:\n","    #print(line)\n","    pass\n","  if len(data[\"skill_with_weight\"])>0:\n","    \n","    d = pd.DataFrame(data[\"skill_with_weight\"], columns=[\"skill_name\",\"skill_value\"])\n","    \n","    df_skills = d if df_skills is None else df_skills.append(d,ignore_index=True)\n","f.close()\n","\n","\n","df_skills = df_skills.groupby(\"skill_name\").agg([\"sum\",\"count\"]).reset_index(level=\"skill_name\")\n","df_skills.columns = [\"_\".join(a) for a in df_skills.columns.to_flat_index()]\n","\n","#remove outlier \n","df_skills = remove_outlier(df_skills)\n","\n","df_skills = df_skills.sort_values([\"skill_name_\"],ascending=True)\n","\n","#output label file\n","df_skills[[\"skill_name_\"]].to_csv(label_path,header=False, index=False)\n","global_label_list = list(df_skills[\"skill_name_\"])\n","\n","# create mapping\n","label2index = dict()\n","index2label = dict()\n","for idx,label in enumerate(global_label_list):\n","  label2index[label]=idx\n","  index2label[idx]=label\n","\n","n_label = len(global_label_list)\n","\n","#output global dataset include text and label\n","\n","dataset = []\n","\n","f = open(extracted_dataset_path, 'rb')\n","# Process extract skill list\n","df_skills = None\n","for idx,line in tqdm(enumerate(f),total=size):\n","  if len(line)==1: #b'\\n'\n","    continue\n","  try:\n","    data = json.loads(line)\n","  except:\n","    print(line)\n","  if len(data[\"skill_with_weight\"])>0:\n","      d = pd.DataFrame(data[\"skill_with_weight\"], columns=[\"skill_name\",\"skill_value\"])\n","      label_list = d[\"skill_name\"]\n","      # create vector for label\n","      label_vector = np.zeros(n_label, dtype=int)\n","      for label in label_list:\n","        if label in global_label_list:\n","          label_vector[label2index[label]] = 1\n","\n","      # create text content\n","      if len(set(label_vector))==2:\n","        for skill_content_pair in data[\"skill_content\"]:\n","              text = skill_content_pair[1]\n","              dataset.append([text] + list(label_vector))\n","\n","# Output dataset for later training\n","pd.DataFrame(dataset).to_csv(output_dataset_path)\n","print(f\"output_dataset_path={output_dataset_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dVqY4nF450_G","executionInfo":{"status":"ok","timestamp":1642964643849,"user_tz":-660,"elapsed":66582,"user":{"displayName":"Dat Huynh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09147954537197073565"}},"outputId":"732cfb7d-8d7a-43be-b41a-c36099a46767"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 33385/33385 [00:26<00:00, 1245.63it/s]\n","100%|██████████| 33385/33385 [00:19<00:00, 1753.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["output_dataset_path=bert_extreme_multilabel_classification/pybert/dataset/seek_dataset/dataset.csv\n"]}]},{"cell_type":"code","source":["print(f\"obtained {len(dataset)} ground true for the dataset\")\n","print(f\"obtained {n_label} labels\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rBktLR6BMdV_","executionInfo":{"status":"ok","timestamp":1642964712171,"user_tz":-660,"elapsed":311,"user":{"displayName":"Dat Huynh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09147954537197073565"}},"outputId":"84de5cea-2661-4e84-bf2e-1856ac243f95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["obtained 17850 ground true for the dataset\n","obtained 2014 labels\n"]}]},{"cell_type":"markdown","source":["#[5. Train a eXtreme Multi-Label Classifier to predict labels for a given JD using BERT framework](#train_bert_xmlc)\n","\n","Given a JD needed for a job, it is required to predict the list of skils/requirements suitable for this job. The following steps are under taken:\n","1. Formulate the problem to a exetreme multi-label classfication problem\n","1. Adapt a base BERT framework for multi-label text classification to train a classifier to predict a list of tags for a given JD\n","\n","**Requirements**:\n","- Place BERT pre-train model into pybert/pretrain/bert/base-uncased/. The pre-trained models and config could be downloaded from [Bert-Multi-Label-Text-Classification](https://github.com/lonePatient/Bert-Multi-Label-Text-Classification)\n","- Check out the config file on pybert/config to make sure the config is correct with the current dataset such as Number of labels."],"metadata":{"id":"R1PUJhcn4J3x"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"ALI8q1PhzGDk"}},{"cell_type":"code","source":["\n","\n","# change to the current directory\n","%cd /content/drive/MyDrive/git/job-skill-prediction/bert_extreme_multilabel_classification\n","\n","# INSTALL REQURIED PACKAGES\n","!pip3 install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"kzqd_VO-YS4W","executionInfo":{"status":"ok","timestamp":1643106501534,"user_tz":-660,"elapsed":18555,"user":{"displayName":"Dat Huynh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09147954537197073565"}},"outputId":"d06f0c1c-a2ee-42bf-bb43-fa3880bb0371"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/git/job-skill-prediction/bert_extreme_multilabel_classification\n","Collecting scikit-learn==0.21.3\n","  Downloading scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\n","\u001b[K     |████████████████████████████████| 6.7 MB 5.1 MB/s \n","\u001b[?25hCollecting pytorch-transformers==1.2.0\n","  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n","\u001b[K     |████████████████████████████████| 176 kB 69.4 MB/s \n","\u001b[?25hCollecting matplotlib==3.1.1\n","  Downloading matplotlib-3.1.1-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n","\u001b[K     |████████████████████████████████| 13.1 MB 63.1 MB/s \n","\u001b[?25hCollecting tensorboard==1.15.0\n","  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 64.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.3->-r requirements.txt (line 1)) (1.19.5)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.3->-r requirements.txt (line 1)) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.3->-r requirements.txt (line 1)) (1.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0->-r requirements.txt (line 2)) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 60.7 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 73.1 MB/s \n","\u001b[?25hCollecting boto3\n","  Downloading boto3-1.20.42-py3-none-any.whl (131 kB)\n","\u001b[K     |████████████████████████████████| 131 kB 73.5 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0->-r requirements.txt (line 2)) (1.10.0+cu111)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0->-r requirements.txt (line 2)) (4.62.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0->-r requirements.txt (line 2)) (2019.12.20)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1->-r requirements.txt (line 3)) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1->-r requirements.txt (line 3)) (1.3.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1->-r requirements.txt (line 3)) (3.0.6)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1->-r requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.15.0->-r requirements.txt (line 4)) (3.17.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.15.0->-r requirements.txt (line 4)) (0.37.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.15.0->-r requirements.txt (line 4)) (3.3.6)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.15.0->-r requirements.txt (line 4)) (0.12.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.15.0->-r requirements.txt (line 4)) (1.0.1)\n","Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.15.0->-r requirements.txt (line 4)) (1.43.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.15.0->-r requirements.txt (line 4)) (1.15.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.15.0->-r requirements.txt (line 4)) (57.4.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard==1.15.0->-r requirements.txt (line 4)) (4.10.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==1.15.0->-r requirements.txt (line 4)) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==1.15.0->-r requirements.txt (line 4)) (3.10.0.2)\n","Collecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting botocore<1.24.0,>=1.23.42\n","  Downloading botocore-1.23.42-py3-none-any.whl (8.5 MB)\n","\u001b[K     |████████████████████████████████| 8.5 MB 54.3 MB/s \n","\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 10.3 MB/s \n","\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 71.8 MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0->-r requirements.txt (line 2)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0->-r requirements.txt (line 2)) (2021.10.8)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 80.7 MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0->-r requirements.txt (line 2)) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers==1.2.0->-r requirements.txt (line 2)) (7.1.2)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, tensorboard, scikit-learn, pytorch-transformers, matplotlib\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.7.0\n","    Uninstalling tensorboard-2.7.0:\n","      Successfully uninstalled tensorboard-2.7.0\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.0.2\n","    Uninstalling scikit-learn-1.0.2:\n","      Successfully uninstalled scikit-learn-1.0.2\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.2.2\n","    Uninstalling matplotlib-3.2.2:\n","      Successfully uninstalled matplotlib-3.2.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n","imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.21.3 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.20.42 botocore-1.23.42 jmespath-0.10.0 matplotlib-3.1.1 pytorch-transformers-1.2.0 s3transfer-0.5.0 sacremoses-0.0.47 scikit-learn-0.21.3 sentencepiece-0.1.96 tensorboard-1.15.0 urllib3-1.25.11\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["matplotlib","mpl_toolkits"]}}},"metadata":{}}]},{"cell_type":"markdown","source":["##5.1 Generating train/validation dataset from the ground-true dataset\n","\n","- Ground true dataset generated in the previous step are in the CSV format, which needs to be pre-processed and convert with label encoded.  \n","- This dataset is required to split into train/validation with 85% vs 15% ratio.\n","- The output is stored at **pybert/dataset/seek_dataset** for further training/validating"],"metadata":{"id":"hSxU9AUmD2JA"}},{"cell_type":"code","source":["!python3 run_bert.py --do_data --data_name seek_dataset --do_lower_case --valid_size 0.15 \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1z9-VHwPDW1_","executionInfo":{"status":"ok","timestamp":1642964757620,"user_tz":-660,"elapsed":19226,"user":{"displayName":"Dat Huynh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09147954537197073565"}},"outputId":"364a8cec-30ea-438d-f33a-51fbee38b201"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training/evaluation parameters Namespace(adam_epsilon=1e-08, arch='bert', data_name='seek_dataset', do_data=True, do_lower_case=True, epochs=10, eval_batch_size=4, eval_max_seq_len=256, fp16=False, fp16_opt_level='O1', grad_clip=1.0, gradient_accumulation_steps=1, learning_rate=0.0001, local_rank=-1, loss_scale=0, mode='min', monitor='valid_loss', n_gpu='0', predict_idx='0', predict_labels=False, resume_path='', save_best=False, seed=42, sorted=1, test=False, test_path='', train=False, train_batch_size=4, train_max_seq_len=256, valid_size=0.15, warmup_proportion=0.1, weight_decay=0.01)\n","split raw data into train and valid\n","[merge] 17850/17850 [==============================] 79.6us/step"]}]},{"cell_type":"markdown","source":["## [5.2 Start training the multi-label classifier to predict skills/requirements/responsbilities](#start_training_bert_xmlc)\n","\n","- Start training with seek_dataset using 100 epochs\n","- Training using GPU to speed up the training time"],"metadata":{"id":"8IKggRbZEt2r"}},{"cell_type":"code","source":["!python3 run_bert.py --train --data_name seek_dataset --do_lower_case --epochs 10 --save_best\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WW-6jlRmY2So","outputId":"eda445ab-0a62-47c5-dd57-e1ce5cfb83c9","executionInfo":{"status":"ok","timestamp":1643026792784,"user_tz":-660,"elapsed":960286,"user":{"displayName":"Dat Huynh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09147954537197073565"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Training/evaluation parameters Namespace(adam_epsilon=1e-08, arch='bert', data_name='seek_dataset', do_data=False, do_lower_case=True, epochs=10, eval_batch_size=4, eval_max_seq_len=256, fp16=False, fp16_opt_level='O1', grad_clip=1.0, gradient_accumulation_steps=1, learning_rate=0.0001, local_rank=-1, loss_scale=0, mode='min', monitor='valid_loss', n_gpu='0', predict_idx='0', predict_labels=False, resume_path='', save_best=False, seed=42, sorted=1, test=False, test_path='', train=True, train_batch_size=4, train_max_seq_len=256, valid_size=0.05, warmup_proportion=0.1, weight_decay=0.01)\n","[create examples] 15172/15172 [==============================] 417.2us/stepSaving examples into cached file pybert/dataset/seek_dataset/cached_train_examples_bert\n","*** Example ***\n","guid: train-0\n","tokens: [CLS] we are looking for someone to focus purely on recruiting permanent staff in the early childhood ed ##uca ##ton sector . the role will include : building relationships with range of existing and prospective clients . identify their issues and help provide the solution . attending client visits and understanding client requirements . representing pulse child care crew as an ambassador at career fairs and other external events . managing recruitment processes to deliver monthly targets . developing and executing your own candidate development strategy . coordinating resources to ensure all va ##can ##cies are properly worked . [SEP]\n","input_ids: 101 2057 2024 2559 2005 2619 2000 3579 11850 2006 14357 4568 3095 1999 1996 2220 5593 3968 18100 2669 4753 1012 1996 2535 2097 2421 1024 2311 6550 2007 2846 1997 4493 1998 17464 7846 1012 6709 2037 3314 1998 2393 3073 1996 5576 1012 7052 7396 7879 1998 4824 7396 5918 1012 5052 8187 2775 2729 3626 2004 2019 6059 2012 2476 21947 1998 2060 6327 2824 1012 6605 15680 6194 2000 8116 7058 7889 1012 4975 1998 23448 2115 2219 4018 2458 5656 1012 19795 4219 2000 5676 2035 12436 9336 9243 2024 7919 2499 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","[create features] 1/15172 [..............................] - ETA: 1:34*** Example ***\n","guid: train-1\n","tokens: [CLS] you will have experience within early education and a passion for business development . to demonstrate ambition , focus , drive , enthusiasm and professional ##ism . desire to become an expert in your area - we pride ourselves on our specialist knowledge . be part of a fast - growing business in an exciting industry . pulse child care crew is a well - established and leading provider of early childhood professionals to child care centres and out of school hours care throughout sydney . we support our clients by placing experienced and compliant early childhood teachers , diploma and certificate ii trained educators as well as cooks and os ##h assistants into permanent and casual roles . our specialist team of consultants come from various early education backgrounds . to find out more about this exciting opportunity please call mandy sheen on 02 99 ##65 94 ##4 for a confidential chat or email your resume to mandy ##at ##child ##care ##cre ##w . com . au . [SEP]\n","input_ids: 101 2017 2097 2031 3325 2306 2220 2495 1998 1037 6896 2005 2449 2458 1012 2000 10580 16290 1010 3579 1010 3298 1010 12024 1998 2658 2964 1012 4792 2000 2468 2019 6739 1999 2115 2181 1011 2057 6620 9731 2006 2256 8325 3716 1012 2022 2112 1997 1037 3435 1011 3652 2449 1999 2019 10990 3068 1012 8187 2775 2729 3626 2003 1037 2092 1011 2511 1998 2877 10802 1997 2220 5593 8390 2000 2775 2729 8941 1998 2041 1997 2082 2847 2729 2802 3994 1012 2057 2490 2256 7846 2011 6885 5281 1998 24577 2220 5593 5089 1010 9827 1998 8196 2462 4738 19156 2004 2092 2004 26929 1998 9808 2232 16838 2046 4568 1998 10017 4395 1012 2256 8325 2136 1997 22283 2272 2013 2536 2220 2495 15406 1012 2000 2424 2041 2062 2055 2023 10990 4495 3531 2655 18193 20682 2006 6185 5585 26187 6365 2549 2005 1037 18777 11834 2030 10373 2115 13746 2000 18193 4017 19339 16302 16748 2860 1012 4012 1012 8740 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","[create features] 15172/15172 [==============================] 1.9ms/stepSaving features into cached file pybert/dataset/seek_dataset/cached_train_features_256_bert\n","sorted data by th length of input\n","[create examples] 2678/2678 [==============================] 383.1us/stepSaving examples into cached file pybert/dataset/seek_dataset/cached_valid_examples_bert\n","*** Example ***\n","guid: valid-0\n","tokens: [CLS] reporting to the ct ##o , you will work with multi - disciplinary teams alongside product managers , architects , developers , data scientists and dev ##ops engineers to deliver production ready solutions that comply with scala ##bility and security standards in line with industry best practices . all developers work across the full stack from the database to the presentation layer , to design and develop convenient , transparent , fast and sim ##pl ##istic solutions for their customers . written mostly in c # and sql , if you are a pro ##active and skilled in software design principles , and have experience with java ##script and framework ##s angular and knockout we would like to hear from you [SEP]\n","input_ids: 101 7316 2000 1996 14931 2080 1010 2017 2097 2147 2007 4800 1011 17972 2780 4077 4031 10489 1010 8160 1010 9797 1010 2951 6529 1998 16475 11923 6145 2000 8116 2537 3201 7300 2008 14037 2007 26743 8553 1998 3036 4781 1999 2240 2007 3068 2190 6078 1012 2035 9797 2147 2408 1996 2440 9991 2013 1996 7809 2000 1996 8312 6741 1010 2000 2640 1998 4503 14057 1010 13338 1010 3435 1998 21934 24759 6553 7300 2005 2037 6304 1012 2517 3262 1999 1039 1001 1998 29296 1010 2065 2017 2024 1037 4013 19620 1998 10571 1999 4007 2640 6481 1010 1998 2031 3325 2007 9262 22483 1998 7705 2015 16108 1998 11369 2057 2052 2066 2000 2963 2013 2017 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","[create features] 1/2678 [..............................] - ETA: 41s*** Example ***\n","guid: valid-1\n","tokens: [CLS] you are naturally in ##qui ##sit ##ive and a problem solve ##r who is willing to share your ideas with others . you are a great com ##mun ##ica ##tor . you are collaborative , inclusive and work well with your colleagues . you have a degree in computer science or software engineering and at least 3 - 5 years commercial software development experience . you have demons ##tra ##ble experience using modern framework ##s , coding standards and techniques . [SEP]\n","input_ids: 101 2017 2024 8100 1999 15549 28032 3512 1998 1037 3291 9611 2099 2040 2003 5627 2000 3745 2115 4784 2007 2500 1012 2017 2024 1037 2307 4012 23041 5555 4263 1012 2017 2024 12317 1010 18678 1998 2147 2092 2007 2115 8628 1012 2017 2031 1037 3014 1999 3274 2671 2030 4007 3330 1998 2012 2560 1017 1011 1019 2086 3293 4007 2458 3325 1012 2017 2031 7942 6494 3468 3325 2478 2715 7705 2015 1010 16861 4781 1998 5461 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","[create features] 2678/2678 [==============================] 2.2ms/stepSaving features into cached file pybert/dataset/seek_dataset/cached_valid_features_256_bert\n","initializing model\n","loading configuration file pybert/pretrain/bert/base-uncased/config.json\n","Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"hidden_size_1\": 2000,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2014,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file pybert/pretrain/bert/base-uncased/pytorch_model.bin\n","Weights of BertForMultiLable not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'classifier_1.weight', 'classifier_1.bias']\n","Weights from pretrained model not used in BertForMultiLable: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","initializing callbacks\n","***** Running training *****\n","  Num examples = 15172\n","  Num Epochs = 10\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 37930\n","model summary info: \n","-----------------------------------------------------------------------\n","             Layer (type)                Input Shape         Param #\n","=======================================================================\n","              BertModel-1                  [-1, 256]               0\n","         BertEmbeddings-2                  [-1, 256]               0\n","              Embedding-3                  [-1, 256]      23,440,896\n","              Embedding-4                  [-1, 256]         393,216\n","              Embedding-5                  [-1, 256]           1,536\n","              LayerNorm-6             [-1, 256, 768]           1,536\n","                Dropout-7             [-1, 256, 768]               0\n","            BertEncoder-8             [-1, 256, 768]               0\n","              BertLayer-9             [-1, 256, 768]               0\n","         BertAttention-10             [-1, 256, 768]               0\n","     BertSelfAttention-11             [-1, 256, 768]               0\n","                Linear-12             [-1, 256, 768]         590,592\n","                Linear-13             [-1, 256, 768]         590,592\n","                Linear-14             [-1, 256, 768]         590,592\n","               Dropout-15         [-1, 12, 256, 256]               0\n","        BertSelfOutput-16             [-1, 256, 768]               0\n","                Linear-17             [-1, 256, 768]         590,592\n","               Dropout-18             [-1, 256, 768]               0\n","             LayerNorm-19             [-1, 256, 768]           1,536\n","      BertIntermediate-20             [-1, 256, 768]               0\n","                Linear-21             [-1, 256, 768]       2,362,368\n","            BertOutput-22            [-1, 256, 3072]               0\n","                Linear-23            [-1, 256, 3072]       2,360,064\n","               Dropout-24             [-1, 256, 768]               0\n","             LayerNorm-25             [-1, 256, 768]           1,536\n","             BertLayer-26             [-1, 256, 768]               0\n","         BertAttention-27             [-1, 256, 768]               0\n","     BertSelfAttention-28             [-1, 256, 768]               0\n","                Linear-29             [-1, 256, 768]         590,592\n","                Linear-30             [-1, 256, 768]         590,592\n","                Linear-31             [-1, 256, 768]         590,592\n","               Dropout-32         [-1, 12, 256, 256]               0\n","        BertSelfOutput-33             [-1, 256, 768]               0\n","                Linear-34             [-1, 256, 768]         590,592\n","               Dropout-35             [-1, 256, 768]               0\n","             LayerNorm-36             [-1, 256, 768]           1,536\n","      BertIntermediate-37             [-1, 256, 768]               0\n","                Linear-38             [-1, 256, 768]       2,362,368\n","            BertOutput-39            [-1, 256, 3072]               0\n","                Linear-40            [-1, 256, 3072]       2,360,064\n","               Dropout-41             [-1, 256, 768]               0\n","             LayerNorm-42             [-1, 256, 768]           1,536\n","             BertLayer-43             [-1, 256, 768]               0\n","         BertAttention-44             [-1, 256, 768]               0\n","     BertSelfAttention-45             [-1, 256, 768]               0\n","                Linear-46             [-1, 256, 768]         590,592\n","                Linear-47             [-1, 256, 768]         590,592\n","                Linear-48             [-1, 256, 768]         590,592\n","               Dropout-49         [-1, 12, 256, 256]               0\n","        BertSelfOutput-50             [-1, 256, 768]               0\n","                Linear-51             [-1, 256, 768]         590,592\n","               Dropout-52             [-1, 256, 768]               0\n","             LayerNorm-53             [-1, 256, 768]           1,536\n","      BertIntermediate-54             [-1, 256, 768]               0\n","                Linear-55             [-1, 256, 768]       2,362,368\n","            BertOutput-56            [-1, 256, 3072]               0\n","                Linear-57            [-1, 256, 3072]       2,360,064\n","               Dropout-58             [-1, 256, 768]               0\n","             LayerNorm-59             [-1, 256, 768]           1,536\n","             BertLayer-60             [-1, 256, 768]               0\n","         BertAttention-61             [-1, 256, 768]               0\n","     BertSelfAttention-62             [-1, 256, 768]               0\n","                Linear-63             [-1, 256, 768]         590,592\n","                Linear-64             [-1, 256, 768]         590,592\n","                Linear-65             [-1, 256, 768]         590,592\n","               Dropout-66         [-1, 12, 256, 256]               0\n","        BertSelfOutput-67             [-1, 256, 768]               0\n","                Linear-68             [-1, 256, 768]         590,592\n","               Dropout-69             [-1, 256, 768]               0\n","             LayerNorm-70             [-1, 256, 768]           1,536\n","      BertIntermediate-71             [-1, 256, 768]               0\n","                Linear-72             [-1, 256, 768]       2,362,368\n","            BertOutput-73            [-1, 256, 3072]               0\n","                Linear-74            [-1, 256, 3072]       2,360,064\n","               Dropout-75             [-1, 256, 768]               0\n","             LayerNorm-76             [-1, 256, 768]           1,536\n","             BertLayer-77             [-1, 256, 768]               0\n","         BertAttention-78             [-1, 256, 768]               0\n","     BertSelfAttention-79             [-1, 256, 768]               0\n","                Linear-80             [-1, 256, 768]         590,592\n","                Linear-81             [-1, 256, 768]         590,592\n","                Linear-82             [-1, 256, 768]         590,592\n","               Dropout-83         [-1, 12, 256, 256]               0\n","        BertSelfOutput-84             [-1, 256, 768]               0\n","                Linear-85             [-1, 256, 768]         590,592\n","               Dropout-86             [-1, 256, 768]               0\n","             LayerNorm-87             [-1, 256, 768]           1,536\n","      BertIntermediate-88             [-1, 256, 768]               0\n","                Linear-89             [-1, 256, 768]       2,362,368\n","            BertOutput-90            [-1, 256, 3072]               0\n","                Linear-91            [-1, 256, 3072]       2,360,064\n","               Dropout-92             [-1, 256, 768]               0\n","             LayerNorm-93             [-1, 256, 768]           1,536\n","             BertLayer-94             [-1, 256, 768]               0\n","         BertAttention-95             [-1, 256, 768]               0\n","     BertSelfAttention-96             [-1, 256, 768]               0\n","                Linear-97             [-1, 256, 768]         590,592\n","                Linear-98             [-1, 256, 768]         590,592\n","                Linear-99             [-1, 256, 768]         590,592\n","              Dropout-100         [-1, 12, 256, 256]               0\n","       BertSelfOutput-101             [-1, 256, 768]               0\n","               Linear-102             [-1, 256, 768]         590,592\n","              Dropout-103             [-1, 256, 768]               0\n","            LayerNorm-104             [-1, 256, 768]           1,536\n","     BertIntermediate-105             [-1, 256, 768]               0\n","               Linear-106             [-1, 256, 768]       2,362,368\n","           BertOutput-107            [-1, 256, 3072]               0\n","               Linear-108            [-1, 256, 3072]       2,360,064\n","              Dropout-109             [-1, 256, 768]               0\n","            LayerNorm-110             [-1, 256, 768]           1,536\n","            BertLayer-111             [-1, 256, 768]               0\n","        BertAttention-112             [-1, 256, 768]               0\n","    BertSelfAttention-113             [-1, 256, 768]               0\n","               Linear-114             [-1, 256, 768]         590,592\n","               Linear-115             [-1, 256, 768]         590,592\n","               Linear-116             [-1, 256, 768]         590,592\n","              Dropout-117         [-1, 12, 256, 256]               0\n","       BertSelfOutput-118             [-1, 256, 768]               0\n","               Linear-119             [-1, 256, 768]         590,592\n","              Dropout-120             [-1, 256, 768]               0\n","            LayerNorm-121             [-1, 256, 768]           1,536\n","     BertIntermediate-122             [-1, 256, 768]               0\n","               Linear-123             [-1, 256, 768]       2,362,368\n","           BertOutput-124            [-1, 256, 3072]               0\n","               Linear-125            [-1, 256, 3072]       2,360,064\n","              Dropout-126             [-1, 256, 768]               0\n","            LayerNorm-127             [-1, 256, 768]           1,536\n","            BertLayer-128             [-1, 256, 768]               0\n","        BertAttention-129             [-1, 256, 768]               0\n","    BertSelfAttention-130             [-1, 256, 768]               0\n","               Linear-131             [-1, 256, 768]         590,592\n","               Linear-132             [-1, 256, 768]         590,592\n","               Linear-133             [-1, 256, 768]         590,592\n","              Dropout-134         [-1, 12, 256, 256]               0\n","       BertSelfOutput-135             [-1, 256, 768]               0\n","               Linear-136             [-1, 256, 768]         590,592\n","              Dropout-137             [-1, 256, 768]               0\n","            LayerNorm-138             [-1, 256, 768]           1,536\n","     BertIntermediate-139             [-1, 256, 768]               0\n","               Linear-140             [-1, 256, 768]       2,362,368\n","           BertOutput-141            [-1, 256, 3072]               0\n","               Linear-142            [-1, 256, 3072]       2,360,064\n","              Dropout-143             [-1, 256, 768]               0\n","            LayerNorm-144             [-1, 256, 768]           1,536\n","            BertLayer-145             [-1, 256, 768]               0\n","        BertAttention-146             [-1, 256, 768]               0\n","    BertSelfAttention-147             [-1, 256, 768]               0\n","               Linear-148             [-1, 256, 768]         590,592\n","               Linear-149             [-1, 256, 768]         590,592\n","               Linear-150             [-1, 256, 768]         590,592\n","              Dropout-151         [-1, 12, 256, 256]               0\n","       BertSelfOutput-152             [-1, 256, 768]               0\n","               Linear-153             [-1, 256, 768]         590,592\n","              Dropout-154             [-1, 256, 768]               0\n","            LayerNorm-155             [-1, 256, 768]           1,536\n","     BertIntermediate-156             [-1, 256, 768]               0\n","               Linear-157             [-1, 256, 768]       2,362,368\n","           BertOutput-158            [-1, 256, 3072]               0\n","               Linear-159            [-1, 256, 3072]       2,360,064\n","              Dropout-160             [-1, 256, 768]               0\n","            LayerNorm-161             [-1, 256, 768]           1,536\n","            BertLayer-162             [-1, 256, 768]               0\n","        BertAttention-163             [-1, 256, 768]               0\n","    BertSelfAttention-164             [-1, 256, 768]               0\n","               Linear-165             [-1, 256, 768]         590,592\n","               Linear-166             [-1, 256, 768]         590,592\n","               Linear-167             [-1, 256, 768]         590,592\n","              Dropout-168         [-1, 12, 256, 256]               0\n","       BertSelfOutput-169             [-1, 256, 768]               0\n","               Linear-170             [-1, 256, 768]         590,592\n","              Dropout-171             [-1, 256, 768]               0\n","            LayerNorm-172             [-1, 256, 768]           1,536\n","     BertIntermediate-173             [-1, 256, 768]               0\n","               Linear-174             [-1, 256, 768]       2,362,368\n","           BertOutput-175            [-1, 256, 3072]               0\n","               Linear-176            [-1, 256, 3072]       2,360,064\n","              Dropout-177             [-1, 256, 768]               0\n","            LayerNorm-178             [-1, 256, 768]           1,536\n","            BertLayer-179             [-1, 256, 768]               0\n","        BertAttention-180             [-1, 256, 768]               0\n","    BertSelfAttention-181             [-1, 256, 768]               0\n","               Linear-182             [-1, 256, 768]         590,592\n","               Linear-183             [-1, 256, 768]         590,592\n","               Linear-184             [-1, 256, 768]         590,592\n","              Dropout-185         [-1, 12, 256, 256]               0\n","       BertSelfOutput-186             [-1, 256, 768]               0\n","               Linear-187             [-1, 256, 768]         590,592\n","              Dropout-188             [-1, 256, 768]               0\n","            LayerNorm-189             [-1, 256, 768]           1,536\n","     BertIntermediate-190             [-1, 256, 768]               0\n","               Linear-191             [-1, 256, 768]       2,362,368\n","           BertOutput-192            [-1, 256, 3072]               0\n","               Linear-193            [-1, 256, 3072]       2,360,064\n","              Dropout-194             [-1, 256, 768]               0\n","            LayerNorm-195             [-1, 256, 768]           1,536\n","            BertLayer-196             [-1, 256, 768]               0\n","        BertAttention-197             [-1, 256, 768]               0\n","    BertSelfAttention-198             [-1, 256, 768]               0\n","               Linear-199             [-1, 256, 768]         590,592\n","               Linear-200             [-1, 256, 768]         590,592\n","               Linear-201             [-1, 256, 768]         590,592\n","              Dropout-202         [-1, 12, 256, 256]               0\n","       BertSelfOutput-203             [-1, 256, 768]               0\n","               Linear-204             [-1, 256, 768]         590,592\n","              Dropout-205             [-1, 256, 768]               0\n","            LayerNorm-206             [-1, 256, 768]           1,536\n","     BertIntermediate-207             [-1, 256, 768]               0\n","               Linear-208             [-1, 256, 768]       2,362,368\n","           BertOutput-209            [-1, 256, 3072]               0\n","               Linear-210            [-1, 256, 3072]       2,360,064\n","              Dropout-211             [-1, 256, 768]               0\n","            LayerNorm-212             [-1, 256, 768]           1,536\n","           BertPooler-213             [-1, 256, 768]               0\n","               Linear-214                  [-1, 768]         590,592\n","                 Tanh-215                  [-1, 768]               0\n","              Dropout-216                  [-1, 768]               0\n","               Linear-217                  [-1, 768]       1,538,000\n","                 ReLU-218                 [-1, 2000]               0\n","               Linear-219                 [-1, 2000]       4,030,014\n","=======================================================================\n","Total params: 115,050,254\n","Trainable params: 5,568,014\n","Non-trainable params: 109,482,240\n","-----------------------------------------------------------------------\n","Epoch 1/10\n","[Training] 3793/3793 [==============================] 43.3ms/step  accuracy: 0.9988 - loss: 0.0090 \n","------------- train result --------------\n","[Evaluating] 670/670 [==============================] 139.1ms/step------------- valid result --------------\n","MRR: 12.633359560872034\n","NDCG@5: 3.6598330057524784\n","NDCG@10: 5.239452388938059\n","NDCG@30: 8.636298873318793\n","NDCG@50: 10.122947273415384\n","NDCG@100: 11.97192152808749\n","Recall@5: 3.516928085511713\n","Recall@10: 6.259900523357516\n","Recall@30: 14.282446669744864\n","Recall@50: 18.727803751817753\n","Recall@100: 25.049701037116268\n","EIM: 126.47063966238723\n","RIIM: 8.23867787653788\n","REIM: 12.654547125906568\n","\n","Epoch: 1 -  valid_loss: 0.0205 \n","\n","Epoch 1: save model to disk.\n","Epoch 2/10\n","[Training] 3793/3793 [==============================] 43.3ms/step  accuracy: 0.9988 - loss: 0.0090 \n","------------- train result --------------\n","[Evaluating] 670/670 [==============================] 138.5ms/step------------- valid result --------------\n","MRR: 17.26611712976612\n","NDCG@5: 5.370984438075344\n","NDCG@10: 6.696504988401338\n","NDCG@30: 10.493329866240458\n","NDCG@50: 11.977609525016153\n","NDCG@100: 14.136428945058405\n","Recall@5: 4.555739201162415\n","Recall@10: 6.844766622748544\n","Recall@30: 16.039540102709605\n","Recall@50: 20.405966978564578\n","Recall@100: 27.798220711793586\n","EIM: 129.51742297616084\n","RIIM: 8.048434505349206\n","REIM: 13.39789428063536\n","\n","Epoch: 2 -  valid_loss: 0.0198 \n","\n","Epoch 2: save model to disk.\n","Epoch 3/10\n","[Training] 3793/3793 [==============================] 43.3ms/step  accuracy: 0.9988 - loss: 0.0081 \n","------------- train result --------------\n","[Evaluating] 670/670 [==============================] 138.5ms/step------------- valid result --------------\n","MRR: 17.952898000799767\n","NDCG@5: 5.472083713036087\n","NDCG@10: 7.112792912764821\n","NDCG@30: 10.720595680115654\n","NDCG@50: 12.27600745834336\n","NDCG@100: 14.59325262882688\n","Recall@5: 4.6106394279341005\n","Recall@10: 7.395440918330401\n","Recall@30: 15.987875724589523\n","Recall@50: 20.6368628977986\n","Recall@100: 28.72548625374342\n","EIM: 131.65174496485176\n","RIIM: 9.151134135794743\n","REIM: 13.756717189886897\n","\n","Epoch: 3 -  valid_loss: 0.0200 \n","\n","Epoch 3: save model to disk.\n","Epoch 4/10\n","[Training] 3793/3793 [==============================] 43.4ms/step  accuracy: 0.9988 - loss: 0.0085 \n","------------- train result --------------\n","[Evaluating] 670/670 [==============================] 140.2ms/step------------- valid result --------------\n","MRR: 18.15244383069738\n","NDCG@5: 5.672080457502442\n","NDCG@10: 7.312373456921412\n","NDCG@30: 10.822205973100225\n","NDCG@50: 12.448709921151394\n","NDCG@100: 14.863782613168219\n","Recall@5: 4.92363934563803\n","Recall@10: 7.788215092564698\n","Recall@30: 16.152832300424404\n","Recall@50: 21.11366177712815\n","Recall@100: 29.408385145491383\n","EIM: 136.20833185153575\n","RIIM: 8.81902046675912\n","REIM: 14.406376011279637\n","\n","Epoch: 4 -  valid_loss: 0.0196 \n","\n","Epoch 4: save model to disk.\n","Epoch 5/10\n","[Training] 3793/3793 [==============================] 43.3ms/step  accuracy: 0.9988 - loss: 0.0083 \n","------------- train result --------------\n","[Evaluating] 670/670 [==============================] 138.9ms/step------------- valid result --------------\n","MRR: 18.156767946251076\n","NDCG@5: 5.742854936343908\n","NDCG@10: 7.433528461172697\n","NDCG@30: 10.851316612495037\n","NDCG@50: 12.639654364175367\n","NDCG@100: 15.140711833883694\n","Recall@5: 5.0269215119593635\n","Recall@10: 8.010844427355034\n","Recall@30: 16.179168016685985\n","Recall@50: 21.547492493663462\n","Recall@100: 30.157565468467435\n","EIM: 142.45997664686985\n","RIIM: 8.314015658939326\n","REIM: 15.266139214144578\n","\n","Epoch: 5 -  valid_loss: 0.0194 \n","\n","Epoch 5: save model to disk.\n","Epoch 6/10\n","[Training] 3793/3793 [==============================] 43.3ms/step  accuracy: 0.9988 - loss: 0.0080 \n","------------- train result --------------\n","[Evaluating] 670/670 [==============================] 137.9ms/step------------- valid result --------------\n","MRR: 17.907616096086894\n","NDCG@5: 5.6578450952293435\n","NDCG@10: 7.464607256070131\n","NDCG@30: 10.801871473918991\n","NDCG@50: 12.649019560066824\n","NDCG@100: 15.13997014809116\n","Recall@5: 4.90917492449939\n","Recall@10: 8.128373442968181\n","Recall@30: 16.157417582193286\n","Recall@50: 21.65188134240705\n","Recall@100: 30.17525306819944\n","EIM: 142.1190594734284\n","RIIM: 8.531562803258561\n","REIM: 15.388927398322014\n","\n","Epoch: 6 -  valid_loss: 0.0193 \n","\n","Epoch 6: save model to disk.\n","Epoch 7/10\n","[Training] 3793/3793 [==============================] 43.3ms/step  accuracy: 0.9988 - loss: 0.0080 \n","------------- train result --------------\n","[Evaluating] 670/670 [==============================] 138.4ms/step------------- valid result --------------\n","MRR: 18.51603091030335\n","NDCG@5: 5.92628624969916\n","NDCG@10: 7.724237128810254\n","NDCG@30: 11.146605416475786\n","NDCG@50: 12.993860671004681\n","NDCG@100: 15.464572246093432\n","Recall@5: 5.089391073160925\n","Recall@10: 8.255909185996398\n","Recall@30: 16.462912350921528\n","Recall@50: 22.00340351856424\n","Recall@100: 30.36712241926736\n","EIM: 143.4143728439845\n","RIIM: 9.2225436814411\n","REIM: 15.544755523658328\n","\n","Epoch: 7 -  valid_loss: 0.0193 \n","\n","Epoch 7: save model to disk.\n","Epoch 8/10\n","[Training] 3793/3793 [==============================] 43.5ms/step  accuracy: 0.9988 - loss: 0.0082 \n","------------- train result --------------\n","[Evaluating] 670/670 [==============================] 138.8ms/step------------- valid result --------------\n","MRR: 19.383842510540774\n","NDCG@5: 6.2530530763960295\n","NDCG@10: 8.07437699724554\n","NDCG@30: 11.684422686628308\n","NDCG@50: 13.443629808972968\n","NDCG@100: 15.962390562296699\n","Recall@5: 5.289065262654969\n","Recall@10: 8.466889835927644\n","Recall@30: 17.163817232423924\n","Recall@50: 22.388474946950183\n","Recall@100: 30.996902369099764\n","EIM: 146.24488779828587\n","RIIM: 9.52786241586746\n","REIM: 15.938495735733206\n","\n","Epoch: 8 -  valid_loss: 0.0192 \n","\n","Epoch 8: save model to disk.\n","Epoch 9/10\n","[Training] 3793/3793 [==============================] 43.3ms/step  accuracy: 0.9988 - loss: 0.0080 \n","------------- train result --------------\n","[Evaluating] 670/670 [==============================] 139.0ms/step------------- valid result --------------\n","MRR: 19.693792202459548\n","NDCG@5: 6.448042686654127\n","NDCG@10: 8.322676059126909\n","NDCG@30: 11.995989219539943\n","NDCG@50: 13.803597489828034\n","NDCG@100: 16.30040849103178\n","Recall@5: 5.492845665757636\n","Recall@10: 8.797484553465026\n","Recall@30: 17.629049168759018\n","Recall@50: 23.10044318268161\n","Recall@100: 31.633533779198252\n","EIM: 147.80513176144242\n","RIIM: 10.150648229219353\n","REIM: 16.089130511484782\n","\n","Epoch: 9 -  valid_loss: 0.0190 \n","\n","Epoch 9: save model to disk.\n","Epoch 10/10\n","[Training] 3793/3793 [==============================] 43.3ms/step  accuracy: 0.9988 - loss: 0.0079 \n","------------- train result --------------\n","[Evaluating] 670/670 [==============================] 138.4ms/step------------- valid result --------------\n","MRR: 20.377048459002594\n","NDCG@5: 6.525759574526026\n","NDCG@10: 8.650778111887641\n","NDCG@30: 12.81365718746384\n","NDCG@50: 14.667622370540126\n","NDCG@100: 17.13125013919646\n","Recall@5: 5.518349070002114\n","Recall@10: 9.206919425707335\n","Recall@30: 19.215348182031533\n","Recall@50: 24.81692619674333\n","Recall@100: 33.26009559669289\n","EIM: 140.70826961603663\n","RIIM: 11.95615952608446\n","REIM: 15.165385562066454\n","\n","Epoch: 10 -  valid_loss: 0.0187 \n","\n","Epoch 10: save model to disk.\n"]}]},{"cell_type":"markdown","source":["# 6. Start predicting on samples from the dataset\n","\n","- The sections below demonstrate real-time predicting on several JD and see the predicted labels\n","\n","## 6.1 Prepare script for predicting"],"metadata":{"id":"MYHLmZGhGjKE"}},{"cell_type":"code","source":["from pybert.test.predictor import Predictor\n","from pybert.io.bert_processor import BertProcessor\n","from torch.utils.data import SequentialSampler\n","from torch.utils.data import DataLoader\n","from pybert.model.nn.bert_for_multi_label import BertForMultiLable\n","from pybert.configs.basic_config import config\n","from pybert.common.tools import init_logger, logger\n","from pathlib import Path\n","\n","\n","import warnings\n","\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Get the processor ready\n","processor = BertProcessor(vocab_path=config['bert_vocab_path'], do_lower_case=False)\n","\n","# get label list ready \n","idx2word = {}\n","for (w,i) in processor.tokenizer.vocab.items():\n","    idx2word[i] = w\n","\n","label_list = processor.get_labels(label_path=config['data_label_path'])\n","\n","idx2label = {i: label for i, label in enumerate(label_list)}\n","\n","\n","\n","# Loading trained model\n","if False:\n","    args.test_path = Path(args.test_path)\n","    model = BertForMultiLable.from_pretrained(args.test_path, num_labels=len(label_list))\n","else:\n","    #model = BertForMultiLable.from_pretrained(config['bert_model_dir'], num_labels=len(label_list))\n","    trained_model_folder = Path(\"/content/drive/MyDrive/git/job-skill-prediction/bert_extreme_multilabel_classification/pybert/output/checkpoints/bert/checkpoint-epoch-10\")\n","    model = BertForMultiLable.from_pretrained(trained_model_folder, num_labels=len(label_list))\n","\n","    \n","for p in model.bert.parameters():\n","    p.require_grad = False\n","\n"],"metadata":{"id":"a2JGMReT8E4I","executionInfo":{"status":"ok","timestamp":1643106520194,"user_tz":-660,"elapsed":18672,"user":{"displayName":"Dat Huynh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09147954537197073565"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path\n","config['test_path'] = Path('pybert/dataset/seek_dataset/seek_dataset.valid.pkl')"],"metadata":{"id":"jYRhvENLbHM3","executionInfo":{"status":"ok","timestamp":1643106525574,"user_tz":-660,"elapsed":369,"user":{"displayName":"Dat Huynh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09147954537197073565"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Get data read to test \n","test_data = processor.get_test(config['test_path'])\n","test_examples = processor.create_examples(lines=test_data, example_type='test', cached_examples_file=config[\n","                                                                    'data_dir'] / f\"cached_test_examples_bert\")\n","test_features = processor.create_features(examples=test_examples, max_seq_len=256, cached_features_file=config[\n","                                                                    'data_dir'] / \"cached_test_features_{}_{}\".format(\n","                                                256, 'bert'\n","                                            ))\n","test_dataset = processor.create_dataset(test_features)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=4)\n","\n","# Start predicting\n","\n","predictor = Predictor(model=model,\n","                      logger=logger,\n","                      n_gpu='0',\n","                      i2w = idx2word,\n","                      i2l = idx2label)\n","\n","\n","result = predictor.predict(data=test_dataloader)\n","\n","\n"," "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V4_nFENFDk5U","executionInfo":{"status":"ok","timestamp":1643106650729,"user_tz":-660,"elapsed":117477,"user":{"displayName":"Dat Huynh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09147954537197073565"}},"outputId":"9b8cea21-09b8-44bb-e82d-b9be045667cd"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[Testing] 670/670 [==============================] 143.3ms/stepMRR: 20.372332387040405\n","NDCG@5: 6.519689674740005\n","NDCG@10: 8.66387888228148\n","NDCG@30: 12.813805196363525\n","NDCG@50: 14.66098760198134\n","NDCG@100: 17.12330679664693\n","Recall@5: 5.508213742843484\n","Recall@10: 9.230181928141176\n","Recall@30: 19.210030039882376\n","Recall@50: 24.776817710548634\n","Recall@100: 33.21054718296621\n","EIM: 140.70826961603663\n","RIIM: 11.954470329340197\n","REIM: 15.165385562066454\n"]}]}]}